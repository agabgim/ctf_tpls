<center><a href="mpishm.c">Actual source code: mpishm.c</a></center><br>

<html>
<head> <link rel="canonical" href="http://www.mcs.anl.gov/petsc/petsc-current/src/sys/utils/mpishm.c.html" />
<title></title>
<meta name="generator" content="c2html 0.9.4">
<meta name="date" content="2020-02-04T16:32:35+00:00">
</head>

<body bgcolor="#FFFFFF">
   <div id="version" align=right><b>petsc-3.12.4 2020-02-04</b></div>
   <div id="bugreport" align=right><a href="mailto:petsc-maint@mcs.anl.gov?subject=Typo or Error in Documentation &body=Please describe the typo or error in the documentation: petsc-3.12.4 v3.12.4 src/sys/utils/mpishm.c.html "><small>Report Typos and Errors</small></a></div>
<pre width="80">
<a name="line1">  1: </a> #include <A href="../../../include/petscsys.h.html">&lt;petscsys.h&gt;</A>
<a name="line2">  2: </a> #include <A href="../../../include/petsc/private/petscimpl.h.html">&lt;petsc/private/petscimpl.h&gt;</A>

<a name="line4">  4: </a><font color="#4169E1"><a name="_n_PetscShmComm"></a>struct _n_PetscShmComm </font>{
<a name="line5">  5: </a>  <a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a> *globranks;       <font color="#B22222">/* global ranks of each rank in the shared memory communicator */</font>
<a name="line6">  6: </a>  <a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a> shmsize;          <font color="#B22222">/* size of the shared memory communicator */</font>
<a name="line7">  7: </a>  <a href="../../../docs/manualpages/Sys/MPI_Comm.html#MPI_Comm">MPI_Comm</a>    globcomm,shmcomm; <font color="#B22222">/* global communicator and shared memory communicator (a sub-communicator of the former) */</font>
<a name="line8">  8: </a>};

<a name="line10"> 10: </a><font color="#B22222">/*</font>
<a name="line11"> 11: </a><font color="#B22222">   Private routine to delete internal tag/name shared memory communicator when a communicator is freed.</font>

<a name="line13"> 13: </a><font color="#B22222">   This is called by MPI, not by users. This is called by <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Comm_free.html#MPI_Comm_free">MPI_Comm_free</a>() when the communicator that has this  data as an attribute is freed.</font>

<a name="line15"> 15: </a><font color="#B22222">   Note: this is declared extern "C" because it is passed to <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Comm_create_keyval.html#MPI_Comm_create_keyval">MPI_Comm_create_keyval</a>()</font>

<a name="line17"> 17: </a><font color="#B22222">*/</font>
<a name="line18"> 18: </a><strong><font color="#4169E1"><a name="Petsc_DelComm_Shm"></a>PETSC_EXTERN <a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a> MPIAPI Petsc_DelComm_Shm(<a href="../../../docs/manualpages/Sys/MPI_Comm.html#MPI_Comm">MPI_Comm</a> comm,<a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a> keyval,void *val,void *extra_state)</font></strong>
<a name="line19"> 19: </a>{
<a name="line20"> 20: </a>  <a href="../../../docs/manualpages/Sys/PetscErrorCode.html#PetscErrorCode">PetscErrorCode</a>  ierr;
<a name="line21"> 21: </a>  PetscShmComm p = (PetscShmComm)val;

<a name="line24"> 24: </a>  PetscInfo1(0,<font color="#666666">"Deleting shared memory subcommunicator in a <a href="../../../docs/manualpages/Sys/MPI_Comm.html#MPI_Comm">MPI_Comm</a> %ld\n"</font>,(long)comm);<a href="../../../docs/manualpages/Sys/CHKERRMPI.html#CHKERRMPI">CHKERRMPI</a>(ierr);
<a name="line25"> 25: </a>  <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Comm_free.html#MPI_Comm_free">MPI_Comm_free</a>(&amp;p-&gt;shmcomm);<a href="../../../docs/manualpages/Sys/CHKERRMPI.html#CHKERRMPI">CHKERRMPI</a>(ierr);
<a name="line26"> 26: </a>  <a href="../../../docs/manualpages/Sys/PetscFree.html#PetscFree">PetscFree</a>(p-&gt;globranks);<a href="../../../docs/manualpages/Sys/CHKERRMPI.html#CHKERRMPI">CHKERRMPI</a>(ierr);
<a name="line27"> 27: </a>  <a href="../../../docs/manualpages/Sys/PetscFree.html#PetscFree">PetscFree</a>(val);<a href="../../../docs/manualpages/Sys/CHKERRMPI.html#CHKERRMPI">CHKERRMPI</a>(ierr);
<a name="line28"> 28: </a>  <a href="../../../docs/manualpages/Sys/PetscFunctionReturn.html#PetscFunctionReturn">PetscFunctionReturn</a>(MPI_SUCCESS);
<a name="line29"> 29: </a>}

<a name="line31"> 31: </a><font color="#B22222">/*@C</font>
<a name="line32"> 32: </a><font color="#B22222">    <a href="../../../docs/manualpages/Sys/PetscShmCommGet.html#PetscShmCommGet">PetscShmCommGet</a> - Given a PETSc communicator returns a communicator of all ranks that share a common memory</font>


<a name="line35"> 35: </a><font color="#B22222">    Collective.</font>

<a name="line37"> 37: </a><font color="#B22222">    Input Parameter:</font>
<a name="line38"> 38: </a><font color="#B22222">.   globcomm - <a href="../../../docs/manualpages/Sys/MPI_Comm.html#MPI_Comm">MPI_Comm</a></font>

<a name="line40"> 40: </a><font color="#B22222">    Output Parameter:</font>
<a name="line41"> 41: </a><font color="#B22222">.   pshmcomm - the PETSc shared memory communicator object</font>

<a name="line43"> 43: </a><font color="#B22222">    Level: developer</font>

<a name="line45"> 45: </a><font color="#B22222">    Notes:</font>
<a name="line46"> 46: </a><font color="#B22222">    This should be called only with an <a href="../../../docs/manualpages/Sys/PetscCommDuplicate.html#PetscCommDuplicate">PetscCommDuplicate</a>() communictor</font>

<a name="line48"> 48: </a><font color="#B22222">           When used with MPICH, MPICH must be configured with --download-mpich-device=ch3:nemesis</font>

<a name="line50"> 50: </a><font color="#B22222">@*/</font>
<a name="line51"> 51: </a><strong><font color="#4169E1"><a name="PetscShmCommGet"></a><a href="../../../docs/manualpages/Sys/PetscErrorCode.html#PetscErrorCode">PetscErrorCode</a> <a href="../../../docs/manualpages/Sys/PetscShmCommGet.html#PetscShmCommGet">PetscShmCommGet</a>(<a href="../../../docs/manualpages/Sys/MPI_Comm.html#MPI_Comm">MPI_Comm</a> globcomm,PetscShmComm *pshmcomm)</font></strong>
<a name="line52"> 52: </a>{
<a name="line53"> 53: </a><font color="#A020F0">#ifdef PETSC_HAVE_MPI_PROCESS_SHARED_MEMORY</font>
<a name="line54"> 54: </a>  <a href="../../../docs/manualpages/Sys/PetscErrorCode.html#PetscErrorCode">PetscErrorCode</a>   ierr;
<a name="line55"> 55: </a>  MPI_Group        globgroup,shmgroup;
<a name="line56"> 56: </a>  <a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a>      *shmranks,i,flg;
<a name="line57"> 57: </a>  PetscCommCounter *counter;

<a name="line60"> 60: </a>  <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Comm_get_attr.html#MPI_Comm_get_attr">MPI_Comm_get_attr</a>(globcomm,Petsc_Counter_keyval,&amp;counter,&amp;flg);
<a name="line61"> 61: </a>  <font color="#4169E1">if</font> (!flg) <a href="../../../docs/manualpages/Sys/SETERRQ.html#SETERRQ">SETERRQ</a>(globcomm,PETSC_ERR_ARG_CORRUPT,<font color="#666666">"Bad MPI communicator supplied; must be a PETSc communicator"</font>);

<a name="line63"> 63: </a>  <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Comm_get_attr.html#MPI_Comm_get_attr">MPI_Comm_get_attr</a>(globcomm,Petsc_ShmComm_keyval,pshmcomm,&amp;flg);
<a name="line64"> 64: </a>  <font color="#4169E1">if</font> (flg) <font color="#4169E1">return</font>(0);

<a name="line66"> 66: </a>  <a href="../../../docs/manualpages/Sys/PetscNew.html#PetscNew">PetscNew</a>(pshmcomm);
<a name="line67"> 67: </a>  (*pshmcomm)-&gt;globcomm = globcomm;

<a name="line69"> 69: </a>  <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Comm_split_type.html#MPI_Comm_split_type">MPI_Comm_split_type</a>(globcomm, MPI_COMM_TYPE_SHARED,0, MPI_INFO_NULL,&amp;(*pshmcomm)-&gt;shmcomm);

<a name="line71"> 71: </a>  <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Comm_size.html#MPI_Comm_size">MPI_Comm_size</a>((*pshmcomm)-&gt;shmcomm,&amp;(*pshmcomm)-&gt;shmsize);
<a name="line72"> 72: </a>  <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Comm_group.html#MPI_Comm_group">MPI_Comm_group</a>(globcomm, &amp;globgroup);
<a name="line73"> 73: </a>  <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Comm_group.html#MPI_Comm_group">MPI_Comm_group</a>((*pshmcomm)-&gt;shmcomm, &amp;shmgroup);
<a name="line74"> 74: </a>  <a href="../../../docs/manualpages/Sys/PetscMalloc1.html#PetscMalloc1">PetscMalloc1</a>((*pshmcomm)-&gt;shmsize,&amp;shmranks);
<a name="line75"> 75: </a>  <a href="../../../docs/manualpages/Sys/PetscMalloc1.html#PetscMalloc1">PetscMalloc1</a>((*pshmcomm)-&gt;shmsize,&amp;(*pshmcomm)-&gt;globranks);
<a name="line76"> 76: </a>  <font color="#4169E1">for</font> (i=0; i&lt;(*pshmcomm)-&gt;shmsize; i++) shmranks[i] = i;
<a name="line77"> 77: </a>  <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Group_translate_ranks.html#MPI_Group_translate_ranks">MPI_Group_translate_ranks</a>(shmgroup, (*pshmcomm)-&gt;shmsize, shmranks, globgroup, (*pshmcomm)-&gt;globranks);
<a name="line78"> 78: </a>  <a href="../../../docs/manualpages/Sys/PetscFree.html#PetscFree">PetscFree</a>(shmranks);
<a name="line79"> 79: </a>  <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Group_free.html#MPI_Group_free">MPI_Group_free</a>(&amp;globgroup);
<a name="line80"> 80: </a>  <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Group_free.html#MPI_Group_free">MPI_Group_free</a>(&amp;shmgroup);

<a name="line82"> 82: </a>  <font color="#4169E1">for</font> (i=0; i&lt;(*pshmcomm)-&gt;shmsize; i++) {
<a name="line83"> 83: </a>    PetscInfo2(NULL,<font color="#666666">"Shared memory rank %d global rank %d\n"</font>,i,(*pshmcomm)-&gt;globranks[i]);
<a name="line84"> 84: </a>  }
<a name="line85"> 85: </a>  <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Comm_set_attr.html#MPI_Comm_set_attr">MPI_Comm_set_attr</a>(globcomm,Petsc_ShmComm_keyval,*pshmcomm);
<a name="line86"> 86: </a>  <font color="#4169E1">return</font>(0);
<a name="line87"> 87: </a><font color="#A020F0">#else</font>
<a name="line88"> 88: </a>  <a href="../../../docs/manualpages/Sys/SETERRQ.html#SETERRQ">SETERRQ</a>(globcomm, PETSC_ERR_SUP, <font color="#666666">"Shared memory communicators need MPI-3 package support.\nPlease upgrade your MPI or reconfigure with --download-mpich."</font>);
<a name="line89"> 89: </a><font color="#A020F0">#endif</font>
<a name="line90"> 90: </a>}

<a name="line92"> 92: </a><font color="#B22222">/*@C</font>
<a name="line93"> 93: </a><font color="#B22222">    <a href="../../../docs/manualpages/Sys/PetscShmCommGlobalToLocal.html#PetscShmCommGlobalToLocal">PetscShmCommGlobalToLocal</a> - Given a global rank returns the local rank in the shared memory communicator</font>

<a name="line95"> 95: </a><font color="#B22222">    Input Parameters:</font>
<a name="line96"> 96: </a><font color="#B22222">+   pshmcomm - the shared memory communicator object</font>
<a name="line97"> 97: </a><font color="#B22222">-   grank    - the global rank</font>

<a name="line99"> 99: </a><font color="#B22222">    Output Parameter:</font>
<a name="line100">100: </a><font color="#B22222">.   lrank - the local rank, or MPI_PROC_NULL if it does not exist</font>

<a name="line102">102: </a><font color="#B22222">    Level: developer</font>

<a name="line104">104: </a><font color="#B22222">    Developer Notes:</font>
<a name="line105">105: </a><font color="#B22222">    Assumes the pshmcomm-&gt;globranks[] is sorted</font>

<a name="line107">107: </a><font color="#B22222">    It may be better to rewrite this to map multiple global ranks to local in the same function call</font>

<a name="line109">109: </a><font color="#B22222">@*/</font>
<a name="line110">110: </a><strong><font color="#4169E1"><a name="PetscShmCommGlobalToLocal"></a><a href="../../../docs/manualpages/Sys/PetscErrorCode.html#PetscErrorCode">PetscErrorCode</a> <a href="../../../docs/manualpages/Sys/PetscShmCommGlobalToLocal.html#PetscShmCommGlobalToLocal">PetscShmCommGlobalToLocal</a>(PetscShmComm pshmcomm,<a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a> grank,<a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a> *lrank)</font></strong>
<a name="line111">111: </a>{
<a name="line112">112: </a>  <a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a>    low,high,t,i;
<a name="line113">113: </a>  <a href="../../../docs/manualpages/Sys/PetscBool.html#PetscBool">PetscBool</a>      flg = <a href="../../../docs/manualpages/Sys/PETSC_FALSE.html#PETSC_FALSE">PETSC_FALSE</a>;

<a name="line117">117: </a>  *lrank = MPI_PROC_NULL;
<a name="line118">118: </a>  <font color="#4169E1">if</font> (grank &lt; pshmcomm-&gt;globranks[0]) <font color="#4169E1">return</font>(0);
<a name="line119">119: </a>  <font color="#4169E1">if</font> (grank &gt; pshmcomm-&gt;globranks[pshmcomm-&gt;shmsize-1]) <font color="#4169E1">return</font>(0);
<a name="line120">120: </a>  <a href="../../../docs/manualpages/Sys/PetscOptionsGetBool.html#PetscOptionsGetBool">PetscOptionsGetBool</a>(NULL,NULL,<font color="#666666">"-noshared"</font>,&amp;flg,NULL);
<a name="line121">121: </a>  <font color="#4169E1">if</font> (flg) <font color="#4169E1">return</font>(0);
<a name="line122">122: </a>  low  = 0;
<a name="line123">123: </a>  high = pshmcomm-&gt;shmsize;
<a name="line124">124: </a>  <font color="#4169E1">while</font> (high-low &gt; 5) {
<a name="line125">125: </a>    t = (low+high)/2;
<a name="line126">126: </a>    <font color="#4169E1">if</font> (pshmcomm-&gt;globranks[t] &gt; grank) high = t;
<a name="line127">127: </a>    <font color="#4169E1">else</font> low = t;
<a name="line128">128: </a>  }
<a name="line129">129: </a>  <font color="#4169E1">for</font> (i=low; i&lt;high; i++) {
<a name="line130">130: </a>    <font color="#4169E1">if</font> (pshmcomm-&gt;globranks[i] &gt; grank) <font color="#4169E1">return</font>(0);
<a name="line131">131: </a>    <font color="#4169E1">if</font> (pshmcomm-&gt;globranks[i] == grank) {
<a name="line132">132: </a>      *lrank = i;
<a name="line133">133: </a>      <font color="#4169E1">return</font>(0);
<a name="line134">134: </a>    }
<a name="line135">135: </a>  }
<a name="line136">136: </a>  <font color="#4169E1">return</font>(0);
<a name="line137">137: </a>}

<a name="line139">139: </a><font color="#B22222">/*@C</font>
<a name="line140">140: </a><font color="#B22222">    <a href="../../../docs/manualpages/Sys/PetscShmCommLocalToGlobal.html#PetscShmCommLocalToGlobal">PetscShmCommLocalToGlobal</a> - Given a local rank in the shared memory communicator returns the global rank</font>

<a name="line142">142: </a><font color="#B22222">    Input Parameters:</font>
<a name="line143">143: </a><font color="#B22222">+   pshmcomm - the shared memory communicator object</font>
<a name="line144">144: </a><font color="#B22222">-   lrank    - the local rank in the shared memory communicator</font>

<a name="line146">146: </a><font color="#B22222">    Output Parameter:</font>
<a name="line147">147: </a><font color="#B22222">.   grank - the global rank in the global communicator where the shared memory communicator is built</font>

<a name="line149">149: </a><font color="#B22222">    Level: developer</font>

<a name="line151">151: </a><font color="#B22222">@*/</font>
<a name="line152">152: </a><strong><font color="#4169E1"><a name="PetscShmCommLocalToGlobal"></a><a href="../../../docs/manualpages/Sys/PetscErrorCode.html#PetscErrorCode">PetscErrorCode</a> <a href="../../../docs/manualpages/Sys/PetscShmCommLocalToGlobal.html#PetscShmCommLocalToGlobal">PetscShmCommLocalToGlobal</a>(PetscShmComm pshmcomm,<a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a> lrank,<a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a> *grank)</font></strong>
<a name="line153">153: </a>{
<a name="line155">155: </a><font color="#A020F0">#ifdef PETSC_USE_DEBUG</font>
<a name="line156">156: </a>  {
<a name="line158">158: </a>    <font color="#4169E1">if</font> (lrank &lt; 0 || lrank &gt;= pshmcomm-&gt;shmsize) { <a href="../../../docs/manualpages/Sys/SETERRQ1.html#SETERRQ1">SETERRQ1</a>(<a href="../../../docs/manualpages/Sys/PETSC_COMM_SELF.html#PETSC_COMM_SELF">PETSC_COMM_SELF</a>,PETSC_ERR_ARG_OUTOFRANGE,<font color="#666666">"No rank %D in the shared memory communicator"</font>,lrank); }
<a name="line159">159: </a>  }
<a name="line160">160: </a><font color="#A020F0">#endif</font>
<a name="line161">161: </a>  *grank = pshmcomm-&gt;globranks[lrank];
<a name="line162">162: </a>  <font color="#4169E1">return</font>(0);
<a name="line163">163: </a>}

<a name="line165">165: </a><font color="#B22222">/*@C</font>
<a name="line166">166: </a><font color="#B22222">    <a href="../../../docs/manualpages/Sys/PetscShmCommGetMpiShmComm.html#PetscShmCommGetMpiShmComm">PetscShmCommGetMpiShmComm</a> - Returns the MPI communicator that represents all processes with common shared memory</font>

<a name="line168">168: </a><font color="#B22222">    Input Parameter:</font>
<a name="line169">169: </a><font color="#B22222">.   pshmcomm - PetscShmComm object obtained with <a href="../../../docs/manualpages/Sys/PetscShmCommGet.html#PetscShmCommGet">PetscShmCommGet</a>()</font>

<a name="line171">171: </a><font color="#B22222">    Output Parameter:</font>
<a name="line172">172: </a><font color="#B22222">.   comm     - the MPI communicator</font>

<a name="line174">174: </a><font color="#B22222">    Level: developer</font>

<a name="line176">176: </a><font color="#B22222">@*/</font>
<a name="line177">177: </a><strong><font color="#4169E1"><a name="PetscShmCommGetMpiShmComm"></a><a href="../../../docs/manualpages/Sys/PetscErrorCode.html#PetscErrorCode">PetscErrorCode</a> <a href="../../../docs/manualpages/Sys/PetscShmCommGetMpiShmComm.html#PetscShmCommGetMpiShmComm">PetscShmCommGetMpiShmComm</a>(PetscShmComm pshmcomm,<a href="../../../docs/manualpages/Sys/MPI_Comm.html#MPI_Comm">MPI_Comm</a> *comm)</font></strong>
<a name="line178">178: </a>{
<a name="line180">180: </a>  *comm = pshmcomm-&gt;shmcomm;
<a name="line181">181: </a>  <font color="#4169E1">return</font>(0);
<a name="line182">182: </a>}

<a name="line184">184: </a><font color="#A020F0">#if defined(PETSC_HAVE_OPENMP_SUPPORT)</font>
<a name="line185">185: </a><font color="#A020F0">#include &lt;pthread.h&gt;</font>
<a name="line186">186: </a><font color="#A020F0">#include &lt;hwloc.h&gt;</font>
<a name="line187">187: </a><font color="#A020F0">#include &lt;omp.h&gt;</font>

<a name="line189">189: </a><font color="#B22222">/* Use mmap() to allocate shared mmeory (for the pthread_barrier_t object) if it is available,</font>
<a name="line190">190: </a><font color="#B22222">   otherwise use <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Win_allocate_shared.html#MPI_Win_allocate_shared">MPI_Win_allocate_shared</a>. They should have the same effect except MPI-3 is much</font>
<a name="line191">191: </a><font color="#B22222">   simpler to use. However, on a Cori Haswell node with Cray MPI, MPI-3 worsened a test's performance</font>
<a name="line192">192: </a><font color="#B22222">   by 50%. Until the reason is found out, we use mmap() instead.</font>
<a name="line193">193: </a><font color="#B22222">*/</font>
<a name="line194">194: </a><strong><font color="#228B22">#define USE_MMAP_ALLOCATE_SHARED_MEMORY</font></strong>

<a name="line196">196: </a><font color="#A020F0">#if defined(USE_MMAP_ALLOCATE_SHARED_MEMORY) &amp;&amp; defined(PETSC_HAVE_MMAP)</font>
<a name="line197">197: </a><font color="#A020F0">#include &lt;sys/mman.h&gt;</font>
<a name="line198">198: </a><font color="#A020F0">#include &lt;sys/types.h&gt;</font>
<a name="line199">199: </a><font color="#A020F0">#include &lt;sys/stat.h&gt;</font>
<a name="line200">200: </a><font color="#A020F0">#include &lt;fcntl.h&gt;</font>
<a name="line201">201: </a><font color="#A020F0">#endif</font>

<a name="line203">203: </a><font color="#4169E1"><a name="_n_PetscOmpCtrl"></a>struct _n_PetscOmpCtrl </font>{
<a name="line204">204: </a>  <a href="../../../docs/manualpages/Sys/MPI_Comm.html#MPI_Comm">MPI_Comm</a>          omp_comm;        <font color="#B22222">/* a shared memory communicator to spawn omp threads */</font>
<a name="line205">205: </a>  <a href="../../../docs/manualpages/Sys/MPI_Comm.html#MPI_Comm">MPI_Comm</a>          omp_master_comm; <font color="#B22222">/* a communicator to give to third party libraries */</font>
<a name="line206">206: </a>  <a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a>       omp_comm_size;   <font color="#B22222">/* size of omp_comm, a kind of OMP_NUM_THREADS */</font>
<a name="line207">207: </a>  <a href="../../../docs/manualpages/Sys/PetscBool.html#PetscBool">PetscBool</a>         is_omp_master;   <font color="#B22222">/* rank 0's in omp_comm */</font>
<a name="line208">208: </a>  MPI_Win           omp_win;         <font color="#B22222">/* a shared memory window containing a barrier */</font>
<a name="line209">209: </a>  pthread_barrier_t *barrier;        <font color="#B22222">/* pointer to the barrier */</font>
<a name="line210">210: </a>  hwloc_topology_t  topology;
<a name="line211">211: </a>  hwloc_cpuset_t    cpuset;          <font color="#B22222">/* cpu bindings of omp master */</font>
<a name="line212">212: </a>  hwloc_cpuset_t    omp_cpuset;      <font color="#B22222">/* union of cpu bindings of ranks in omp_comm */</font>
<a name="line213">213: </a>};


<a name="line216">216: </a><font color="#B22222">/* Allocate and initialize a pthread_barrier_t object in memory shared by processes in omp_comm</font>
<a name="line217">217: </a><font color="#B22222">   contained by the controller.</font>

<a name="line219">219: </a><font color="#B22222">   PETSc OpenMP controller users do not call this function directly. This function exists</font>
<a name="line220">220: </a><font color="#B22222">   only because we want to separate shared memory allocation methods from other code.</font>
<a name="line221">221: </a><font color="#B22222"> */</font>
<a name="line222">222: </a><strong><font color="#4169E1"><a name="PetscOmpCtrlCreateBarrier"></a>PETSC_STATIC_INLINE <a href="../../../docs/manualpages/Sys/PetscErrorCode.html#PetscErrorCode">PetscErrorCode</a> PetscOmpCtrlCreateBarrier(PetscOmpCtrl ctrl)</font></strong>
<a name="line223">223: </a>{
<a name="line224">224: </a>  <a href="../../../docs/manualpages/Sys/PetscErrorCode.html#PetscErrorCode">PetscErrorCode</a>        ierr;
<a name="line225">225: </a>  MPI_Aint              size;
<a name="line226">226: </a>  void                  *baseptr;
<a name="line227">227: </a>  pthread_barrierattr_t  attr;

<a name="line229">229: </a><font color="#A020F0">#if defined(USE_MMAP_ALLOCATE_SHARED_MEMORY) &amp;&amp; defined(PETSC_HAVE_MMAP)</font>
<a name="line230">230: </a>  <a href="../../../docs/manualpages/Sys/PetscInt.html#PetscInt">PetscInt</a>              fd;
<a name="line231">231: </a>  PetscChar             pathname[PETSC_MAX_PATH_LEN];
<a name="line232">232: </a><font color="#A020F0">#else</font>
<a name="line233">233: </a>  <a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a>           disp_unit;
<a name="line234">234: </a><font color="#A020F0">#endif</font>

<a name="line237">237: </a><font color="#A020F0">#if defined(USE_MMAP_ALLOCATE_SHARED_MEMORY) &amp;&amp; defined(PETSC_HAVE_MMAP)</font>
<a name="line238">238: </a>  size = <font color="#4169E1">sizeof</font>(pthread_barrier_t);
<a name="line239">239: </a>  <font color="#4169E1">if</font> (ctrl-&gt;is_omp_master) {
<a name="line240">240: </a>    <font color="#B22222">/* use <a href="../../../docs/manualpages/Sys/PETSC_COMM_SELF.html#PETSC_COMM_SELF">PETSC_COMM_SELF</a> in <a href="../../../docs/manualpages/Sys/PetscGetTmp.html#PetscGetTmp">PetscGetTmp</a>, since it is a collective call. Using omp_comm would otherwise bcast the partially populated pathname to slaves */</font>
<a name="line241">241: </a>    <a href="../../../docs/manualpages/Sys/PetscGetTmp.html#PetscGetTmp">PetscGetTmp</a>(<a href="../../../docs/manualpages/Sys/PETSC_COMM_SELF.html#PETSC_COMM_SELF">PETSC_COMM_SELF</a>,pathname,PETSC_MAX_PATH_LEN);
<a name="line242">242: </a>    <a href="../../../docs/manualpages/Sys/PetscStrlcat.html#PetscStrlcat">PetscStrlcat</a>(pathname,<font color="#666666">"/petsc-shm-XXXXXX"</font>,PETSC_MAX_PATH_LEN);
<a name="line243">243: </a>    <font color="#B22222">/* mkstemp replaces XXXXXX with a unique file name and opens the file for us */</font>
<a name="line244">244: </a>    fd      = mkstemp(pathname); <font color="#4169E1">if</font>(fd == -1) <a href="../../../docs/manualpages/Sys/SETERRQ1.html#SETERRQ1">SETERRQ1</a>(<a href="../../../docs/manualpages/Sys/PETSC_COMM_SELF.html#PETSC_COMM_SELF">PETSC_COMM_SELF</a>,PETSC_ERR_LIB,<font color="#666666">"Could not create tmp file %s with mkstemp\n"</font>, pathname);
<a name="line245">245: </a>    ftruncate(fd,size);
<a name="line246">246: </a>    baseptr = mmap(NULL,size,PROT_READ | PROT_WRITE, MAP_SHARED,fd,0); <font color="#4169E1">if</font> (baseptr == MAP_FAILED) <a href="../../../docs/manualpages/Sys/SETERRQ.html#SETERRQ">SETERRQ</a>(<a href="../../../docs/manualpages/Sys/PETSC_COMM_SELF.html#PETSC_COMM_SELF">PETSC_COMM_SELF</a>,PETSC_ERR_LIB,<font color="#666666">"mmap() failed\n"</font>);
<a name="line247">247: </a>    close(fd);
<a name="line248">248: </a>    <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Bcast.html#MPI_Bcast">MPI_Bcast</a>(pathname,PETSC_MAX_PATH_LEN,MPI_CHAR,0,ctrl-&gt;omp_comm);
<a name="line249">249: </a>    <font color="#B22222">/* this <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Barrier.html#MPI_Barrier">MPI_Barrier</a> is to wait slaves to open the file before master unlinks it */</font>
<a name="line250">250: </a>    <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Barrier.html#MPI_Barrier">MPI_Barrier</a>(ctrl-&gt;omp_comm);
<a name="line251">251: </a>    unlink(pathname);
<a name="line252">252: </a>  } <font color="#4169E1">else</font> {
<a name="line253">253: </a>    <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Bcast.html#MPI_Bcast">MPI_Bcast</a>(pathname,PETSC_MAX_PATH_LEN,MPI_CHAR,0,ctrl-&gt;omp_comm);
<a name="line254">254: </a>    fd      = open(pathname,O_RDWR); <font color="#4169E1">if</font>(fd == -1) <a href="../../../docs/manualpages/Sys/SETERRQ1.html#SETERRQ1">SETERRQ1</a>(<a href="../../../docs/manualpages/Sys/PETSC_COMM_SELF.html#PETSC_COMM_SELF">PETSC_COMM_SELF</a>,PETSC_ERR_LIB,<font color="#666666">"Could not open tmp file %s\n"</font>, pathname);
<a name="line255">255: </a>    baseptr = mmap(NULL,size,PROT_READ | PROT_WRITE, MAP_SHARED,fd,0); <font color="#4169E1">if</font> (baseptr == MAP_FAILED) <a href="../../../docs/manualpages/Sys/SETERRQ.html#SETERRQ">SETERRQ</a>(<a href="../../../docs/manualpages/Sys/PETSC_COMM_SELF.html#PETSC_COMM_SELF">PETSC_COMM_SELF</a>,PETSC_ERR_LIB,<font color="#666666">"mmap() failed\n"</font>);
<a name="line256">256: </a>    close(fd);
<a name="line257">257: </a>    <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Barrier.html#MPI_Barrier">MPI_Barrier</a>(ctrl-&gt;omp_comm);
<a name="line258">258: </a>  }
<a name="line259">259: </a><font color="#A020F0">#else</font>
<a name="line260">260: </a>  size = ctrl-&gt;is_omp_master ? <font color="#4169E1">sizeof</font>(pthread_barrier_t) : 0;
<a name="line261">261: </a>  <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Win_allocate_shared.html#MPI_Win_allocate_shared">MPI_Win_allocate_shared</a>(size,1,MPI_INFO_NULL,ctrl-&gt;omp_comm,&amp;baseptr,&amp;ctrl-&gt;omp_win);
<a name="line262">262: </a>  <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Win_shared_query.html#MPI_Win_shared_query">MPI_Win_shared_query</a>(ctrl-&gt;omp_win,0,&amp;size,&amp;disp_unit,&amp;baseptr);
<a name="line263">263: </a><font color="#A020F0">#endif</font>
<a name="line264">264: </a>  ctrl-&gt;barrier = (pthread_barrier_t*)baseptr;

<a name="line266">266: </a>  <font color="#B22222">/* omp master initializes the barrier */</font>
<a name="line267">267: </a>  <font color="#4169E1">if</font> (ctrl-&gt;is_omp_master) {
<a name="line268">268: </a>    <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Comm_size.html#MPI_Comm_size">MPI_Comm_size</a>(ctrl-&gt;omp_comm,&amp;ctrl-&gt;omp_comm_size);
<a name="line269">269: </a>    pthread_barrierattr_init(&amp;attr);
<a name="line270">270: </a>    pthread_barrierattr_setpshared(&amp;attr,PTHREAD_PROCESS_SHARED); <font color="#B22222">/* make the barrier also work for processes */</font>
<a name="line271">271: </a>    pthread_barrier_init(ctrl-&gt;barrier,&amp;attr,(unsigned int)ctrl-&gt;omp_comm_size);
<a name="line272">272: </a>    pthread_barrierattr_destroy(&amp;attr);
<a name="line273">273: </a>  }

<a name="line275">275: </a>  <font color="#B22222">/* this <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Barrier.html#MPI_Barrier">MPI_Barrier</a> is to make sure the omp barrier is initialized before slaves use it */</font>
<a name="line276">276: </a>  <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Barrier.html#MPI_Barrier">MPI_Barrier</a>(ctrl-&gt;omp_comm);
<a name="line277">277: </a>  <font color="#4169E1">return</font>(0);
<a name="line278">278: </a>}

<a name="line280">280: </a><font color="#B22222">/* Destroy the pthread barrier in the PETSc OpenMP controller */</font>
<a name="line281">281: </a><strong><font color="#4169E1"><a name="PetscOmpCtrlDestroyBarrier"></a>PETSC_STATIC_INLINE <a href="../../../docs/manualpages/Sys/PetscErrorCode.html#PetscErrorCode">PetscErrorCode</a> PetscOmpCtrlDestroyBarrier(PetscOmpCtrl ctrl)</font></strong>
<a name="line282">282: </a>{

<a name="line286">286: </a>  <font color="#B22222">/* this <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Barrier.html#MPI_Barrier">MPI_Barrier</a> is to make sure slaves have finished using the omp barrier before master destroys it */</font>
<a name="line287">287: </a>  <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Barrier.html#MPI_Barrier">MPI_Barrier</a>(ctrl-&gt;omp_comm);
<a name="line288">288: </a>  <font color="#4169E1">if</font> (ctrl-&gt;is_omp_master) { pthread_barrier_destroy(ctrl-&gt;barrier); }

<a name="line290">290: </a><font color="#A020F0">#if defined(USE_MMAP_ALLOCATE_SHARED_MEMORY) &amp;&amp; defined(PETSC_HAVE_MMAP)</font>
<a name="line291">291: </a>  munmap(ctrl-&gt;barrier,<font color="#4169E1">sizeof</font>(pthread_barrier_t));
<a name="line292">292: </a><font color="#A020F0">#else</font>
<a name="line293">293: </a>  <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Win_free.html#MPI_Win_free">MPI_Win_free</a>(&amp;ctrl-&gt;omp_win);
<a name="line294">294: </a><font color="#A020F0">#endif</font>
<a name="line295">295: </a>  <font color="#4169E1">return</font>(0);
<a name="line296">296: </a>}

<a name="line298">298: </a><font color="#B22222">/*@C</font>
<a name="line299">299: </a><font color="#B22222">    <a href="../../../docs/manualpages/Sys/PetscOmpCtrlCreate.html#PetscOmpCtrlCreate">PetscOmpCtrlCreate</a> - create a PETSc OpenMP controller, which manages PETSc's interaction with third party libraries using OpenMP</font>

<a name="line301">301: </a><font color="#B22222">    Input Parameter:</font>
<a name="line302">302: </a><font color="#B22222">+   petsc_comm - a communicator some PETSc object (for example, a matrix) lives in</font>
<a name="line303">303: </a><font color="#B22222">-   nthreads   - number of threads per MPI rank to spawn in a library using OpenMP. If nthreads = -1, let PETSc decide a suitable value</font>

<a name="line305">305: </a><font color="#B22222">    Output Parameter:</font>
<a name="line306">306: </a><font color="#B22222">.   pctrl      - a PETSc OpenMP controller</font>

<a name="line308">308: </a><font color="#B22222">    Level: developer</font>

<a name="line310">310: </a><font color="#B22222">    TODO: Possibly use the variable PetscNumOMPThreads to determine the number for threads to use </font>

<a name="line312">312: </a><font color="#B22222">.seealso <a href="../../../docs/manualpages/Sys/PetscOmpCtrlDestroy.html#PetscOmpCtrlDestroy">PetscOmpCtrlDestroy</a>()</font>
<a name="line313">313: </a><font color="#B22222">@*/</font>
<a name="line314">314: </a><strong><font color="#4169E1"><a name="PetscOmpCtrlCreate"></a><a href="../../../docs/manualpages/Sys/PetscErrorCode.html#PetscErrorCode">PetscErrorCode</a> <a href="../../../docs/manualpages/Sys/PetscOmpCtrlCreate.html#PetscOmpCtrlCreate">PetscOmpCtrlCreate</a>(<a href="../../../docs/manualpages/Sys/MPI_Comm.html#MPI_Comm">MPI_Comm</a> petsc_comm,<a href="../../../docs/manualpages/Sys/PetscInt.html#PetscInt">PetscInt</a> nthreads,PetscOmpCtrl *pctrl)</font></strong>
<a name="line315">315: </a>{
<a name="line316">316: </a>  <a href="../../../docs/manualpages/Sys/PetscErrorCode.html#PetscErrorCode">PetscErrorCode</a>        ierr;
<a name="line317">317: </a>  PetscOmpCtrl          ctrl;
<a name="line318">318: </a>  unsigned long         *cpu_ulongs=NULL;
<a name="line319">319: </a>  <a href="../../../docs/manualpages/Sys/PetscInt.html#PetscInt">PetscInt</a>              i,nr_cpu_ulongs;
<a name="line320">320: </a>  PetscShmComm          pshmcomm;
<a name="line321">321: </a>  <a href="../../../docs/manualpages/Sys/MPI_Comm.html#MPI_Comm">MPI_Comm</a>              shm_comm;
<a name="line322">322: </a>  <a href="../../../docs/manualpages/Sys/PetscMPIInt.html#PetscMPIInt">PetscMPIInt</a>           shm_rank,shm_comm_size,omp_rank,color;
<a name="line323">323: </a>  <a href="../../../docs/manualpages/Sys/PetscInt.html#PetscInt">PetscInt</a>              num_packages,num_cores;

<a name="line326">326: </a>  <a href="../../../docs/manualpages/Sys/PetscNew.html#PetscNew">PetscNew</a>(&amp;ctrl);

<a name="line328">328: </a>  <font color="#B22222">/*=================================================================================</font>
<a name="line329">329: </a><font color="#B22222">    Init hwloc</font>
<a name="line330">330: </a><font color="#B22222">   ==================================================================================*/</font>
<a name="line331">331: </a>  hwloc_topology_init(&amp;ctrl-&gt;topology);
<a name="line332">332: </a><font color="#A020F0">#if HWLOC_API_VERSION &gt;= 0x00020000</font>
<a name="line333">333: </a>  <font color="#B22222">/* to filter out unneeded info and have faster hwloc_topology_load */</font>
<a name="line334">334: </a>  hwloc_topology_set_all_types_filter(ctrl-&gt;topology,HWLOC_TYPE_FILTER_KEEP_NONE);
<a name="line335">335: </a>  hwloc_topology_set_type_filter(ctrl-&gt;topology,HWLOC_OBJ_CORE,HWLOC_TYPE_FILTER_KEEP_ALL);
<a name="line336">336: </a><font color="#A020F0">#endif</font>
<a name="line337">337: </a>  hwloc_topology_load(ctrl-&gt;topology);

<a name="line339">339: </a>  <font color="#B22222">/*=================================================================================</font>
<a name="line340">340: </a><font color="#B22222">    Split petsc_comm into multiple omp_comms. Ranks in an omp_comm have access to</font>
<a name="line341">341: </a><font color="#B22222">    physically shared memory. Rank 0 of each omp_comm is called an OMP master, and</font>
<a name="line342">342: </a><font color="#B22222">    others are called slaves. OMP Masters make up a new comm called omp_master_comm,</font>
<a name="line343">343: </a><font color="#B22222">    which is usually passed to third party libraries.</font>
<a name="line344">344: </a><font color="#B22222">   ==================================================================================*/</font>

<a name="line346">346: </a>  <font color="#B22222">/* fetch the stored shared memory communicator */</font>
<a name="line347">347: </a>  <a href="../../../docs/manualpages/Sys/PetscShmCommGet.html#PetscShmCommGet">PetscShmCommGet</a>(petsc_comm,&amp;pshmcomm);
<a name="line348">348: </a>  <a href="../../../docs/manualpages/Sys/PetscShmCommGetMpiShmComm.html#PetscShmCommGetMpiShmComm">PetscShmCommGetMpiShmComm</a>(pshmcomm,&amp;shm_comm);

<a name="line350">350: </a>  <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Comm_rank.html#MPI_Comm_rank">MPI_Comm_rank</a>(shm_comm,&amp;shm_rank);
<a name="line351">351: </a>  <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Comm_size.html#MPI_Comm_size">MPI_Comm_size</a>(shm_comm,&amp;shm_comm_size);

<a name="line353">353: </a>  <font color="#B22222">/* PETSc decides nthreads, which is the smaller of shm_comm_size or cores per package(socket) */</font>
<a name="line354">354: </a>  <font color="#4169E1">if</font> (nthreads == -1) {
<a name="line355">355: </a>    num_packages = hwloc_get_nbobjs_by_type(ctrl-&gt;topology,HWLOC_OBJ_PACKAGE); <font color="#4169E1">if</font> (num_packages &lt;= 0) <a href="../../../docs/manualpages/Sys/SETERRQ.html#SETERRQ">SETERRQ</a>(<a href="../../../docs/manualpages/Sys/PETSC_COMM_SELF.html#PETSC_COMM_SELF">PETSC_COMM_SELF</a>,PETSC_ERR_LIB,<font color="#666666">"Could not determine number of sockets(packages) per compute node\n"</font>);
<a name="line356">356: </a>    num_cores    = hwloc_get_nbobjs_by_type(ctrl-&gt;topology,HWLOC_OBJ_CORE);    <font color="#4169E1">if</font> (num_cores    &lt;= 0) <a href="../../../docs/manualpages/Sys/SETERRQ.html#SETERRQ">SETERRQ</a>(<a href="../../../docs/manualpages/Sys/PETSC_COMM_SELF.html#PETSC_COMM_SELF">PETSC_COMM_SELF</a>,PETSC_ERR_LIB,<font color="#666666">"Could not determine number of cores per compute node\n"</font>);
<a name="line357">357: </a>    nthreads     = num_cores/num_packages;
<a name="line358">358: </a>    <font color="#4169E1">if</font> (nthreads &gt; shm_comm_size) nthreads = shm_comm_size;
<a name="line359">359: </a>  }

<a name="line361">361: </a>  <font color="#4169E1">if</font> (nthreads &lt; 1 || nthreads &gt; shm_comm_size) <a href="../../../docs/manualpages/Sys/SETERRQ2.html#SETERRQ2">SETERRQ2</a>(petsc_comm,PETSC_ERR_ARG_OUTOFRANGE,<font color="#666666">"number of OpenMP threads %d can not be &lt; 1 or &gt; the MPI shared memory communicator size %d\n"</font>,nthreads,shm_comm_size);
<a name="line362">362: </a>  <font color="#4169E1">if</font> (shm_comm_size % nthreads) { <a href="../../../docs/manualpages/Sys/PetscPrintf.html#PetscPrintf">PetscPrintf</a>(petsc_comm,<font color="#666666">"Warning: number of OpenMP threads %d is not a factor of the MPI shared memory communicator size %d, which may cause load-imbalance!\n"</font>,nthreads,shm_comm_size); }

<a name="line364">364: </a>  <font color="#B22222">/* split shm_comm into a set of omp_comms with each of size nthreads. Ex., if</font>
<a name="line365">365: </a><font color="#B22222">     shm_comm_size=16, nthreads=8, then ranks 0~7 get color 0 and ranks 8~15 get</font>
<a name="line366">366: </a><font color="#B22222">     color 1. They are put in two omp_comms. Note that petsc_ranks may or may not</font>
<a name="line367">367: </a><font color="#B22222">     be consecutive in a shm_comm, but shm_ranks always run from 0 to shm_comm_size-1.</font>
<a name="line368">368: </a><font color="#B22222">     Use 0 as key so that rank ordering wont change in new comm.</font>
<a name="line369">369: </a><font color="#B22222">   */</font>
<a name="line370">370: </a>  color = shm_rank / nthreads;
<a name="line371">371: </a>  <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Comm_split.html#MPI_Comm_split">MPI_Comm_split</a>(shm_comm,color,0<font color="#B22222">/*key*/</font>,&amp;ctrl-&gt;omp_comm);

<a name="line373">373: </a>  <font color="#B22222">/* put rank 0's in omp_comms (i.e., master ranks) into a new comm - omp_master_comm */</font>
<a name="line374">374: </a>  <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Comm_rank.html#MPI_Comm_rank">MPI_Comm_rank</a>(ctrl-&gt;omp_comm,&amp;omp_rank);
<a name="line375">375: </a>  <font color="#4169E1">if</font> (!omp_rank) {
<a name="line376">376: </a>    ctrl-&gt;is_omp_master = <a href="../../../docs/manualpages/Sys/PETSC_TRUE.html#PETSC_TRUE">PETSC_TRUE</a>;  <font color="#B22222">/* master */</font>
<a name="line377">377: </a>    color = 0;
<a name="line378">378: </a>  } <font color="#4169E1">else</font> {
<a name="line379">379: </a>    ctrl-&gt;is_omp_master = <a href="../../../docs/manualpages/Sys/PETSC_FALSE.html#PETSC_FALSE">PETSC_FALSE</a>; <font color="#B22222">/* slave */</font>
<a name="line380">380: </a>    color = MPI_UNDEFINED; <font color="#B22222">/* to make slaves get omp_master_comm = MPI_COMM_NULL in <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Comm_split.html#MPI_Comm_split">MPI_Comm_split</a> */</font>
<a name="line381">381: </a>  }
<a name="line382">382: </a>  <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Comm_split.html#MPI_Comm_split">MPI_Comm_split</a>(petsc_comm,color,0<font color="#B22222">/*key*/</font>,&amp;ctrl-&gt;omp_master_comm); <font color="#B22222">/* rank 0 in omp_master_comm is rank 0 in petsc_comm */</font>

<a name="line384">384: </a>  <font color="#B22222">/*=================================================================================</font>
<a name="line385">385: </a><font color="#B22222">    Each omp_comm has a pthread_barrier_t in its shared memory, which is used to put</font>
<a name="line386">386: </a><font color="#B22222">    slave ranks in sleep and idle their CPU, so that the master can fork OMP threads</font>
<a name="line387">387: </a><font color="#B22222">    and run them on the idle CPUs.</font>
<a name="line388">388: </a><font color="#B22222">   ==================================================================================*/</font>
<a name="line389">389: </a>  PetscOmpCtrlCreateBarrier(ctrl);

<a name="line391">391: </a>  <font color="#B22222">/*=================================================================================</font>
<a name="line392">392: </a><font color="#B22222">    omp master logs its cpu binding (i.e., cpu set) and computes a new binding that</font>
<a name="line393">393: </a><font color="#B22222">    is the union of the bindings of all ranks in the omp_comm</font>
<a name="line394">394: </a><font color="#B22222">    =================================================================================*/</font>

<a name="line396">396: </a>  ctrl-&gt;cpuset = hwloc_bitmap_alloc(); <font color="#4169E1">if</font> (!ctrl-&gt;cpuset) <a href="../../../docs/manualpages/Sys/SETERRQ.html#SETERRQ">SETERRQ</a>(<a href="../../../docs/manualpages/Sys/PETSC_COMM_SELF.html#PETSC_COMM_SELF">PETSC_COMM_SELF</a>,PETSC_ERR_LIB,<font color="#666666">"hwloc_bitmap_alloc() failed\n"</font>);
<a name="line397">397: </a>  hwloc_get_cpubind(ctrl-&gt;topology,ctrl-&gt;cpuset, HWLOC_CPUBIND_PROCESS);

<a name="line399">399: </a>  <font color="#B22222">/* hwloc main developer said they will add new APIs hwloc_bitmap_{nr,to,from}_ulongs in 2.1 to help us simplify the following bitmap pack/unpack code */</font>
<a name="line400">400: </a>  nr_cpu_ulongs = (hwloc_bitmap_last(hwloc_topology_get_topology_cpuset (ctrl-&gt;topology))+<font color="#4169E1">sizeof</font>(unsigned long)*8)/<font color="#4169E1">sizeof</font>(unsigned long)/8;
<a name="line401">401: </a>  <a href="../../../docs/manualpages/Sys/PetscMalloc1.html#PetscMalloc1">PetscMalloc1</a>(nr_cpu_ulongs,&amp;cpu_ulongs);
<a name="line402">402: </a>  <font color="#4169E1">if</font> (nr_cpu_ulongs == 1) {
<a name="line403">403: </a>    cpu_ulongs[0] = hwloc_bitmap_to_ulong(ctrl-&gt;cpuset);
<a name="line404">404: </a>  } <font color="#4169E1">else</font> {
<a name="line405">405: </a>    <font color="#4169E1">for</font> (i=0; i&lt;nr_cpu_ulongs; i++) cpu_ulongs[i] = hwloc_bitmap_to_ith_ulong(ctrl-&gt;cpuset,(unsigned)i);
<a name="line406">406: </a>  }

<a name="line408">408: </a>  <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Reduce.html#MPI_Reduce">MPI_Reduce</a>(ctrl-&gt;is_omp_master ? MPI_IN_PLACE : cpu_ulongs, cpu_ulongs,nr_cpu_ulongs, MPI_UNSIGNED_LONG,MPI_BOR,0,ctrl-&gt;omp_comm);

<a name="line410">410: </a>  <font color="#4169E1">if</font> (ctrl-&gt;is_omp_master) {
<a name="line411">411: </a>    ctrl-&gt;omp_cpuset = hwloc_bitmap_alloc(); <font color="#4169E1">if</font> (!ctrl-&gt;omp_cpuset) <a href="../../../docs/manualpages/Sys/SETERRQ.html#SETERRQ">SETERRQ</a>(<a href="../../../docs/manualpages/Sys/PETSC_COMM_SELF.html#PETSC_COMM_SELF">PETSC_COMM_SELF</a>,PETSC_ERR_LIB,<font color="#666666">"hwloc_bitmap_alloc() failed\n"</font>);
<a name="line412">412: </a>    <font color="#4169E1">if</font> (nr_cpu_ulongs == 1) {
<a name="line413">413: </a><font color="#A020F0">#if HWLOC_API_VERSION &gt;= 0x00020000</font>
<a name="line414">414: </a>      hwloc_bitmap_from_ulong(ctrl-&gt;omp_cpuset,cpu_ulongs[0]);
<a name="line415">415: </a><font color="#A020F0">#else</font>
<a name="line416">416: </a>      hwloc_bitmap_from_ulong(ctrl-&gt;omp_cpuset,cpu_ulongs[0]);
<a name="line417">417: </a><font color="#A020F0">#endif</font>
<a name="line418">418: </a>    } <font color="#4169E1">else</font> {
<a name="line419">419: </a>      <font color="#4169E1">for</font> (i=0; i&lt;nr_cpu_ulongs; i++)  {
<a name="line420">420: </a><font color="#A020F0">#if HWLOC_API_VERSION &gt;= 0x00020000</font>
<a name="line421">421: </a>        hwloc_bitmap_set_ith_ulong(ctrl-&gt;omp_cpuset,(unsigned)i,cpu_ulongs[i]);
<a name="line422">422: </a><font color="#A020F0">#else</font>
<a name="line423">423: </a>        hwloc_bitmap_set_ith_ulong(ctrl-&gt;omp_cpuset,(unsigned)i,cpu_ulongs[i]);
<a name="line424">424: </a><font color="#A020F0">#endif</font>
<a name="line425">425: </a>      }
<a name="line426">426: </a>    }
<a name="line427">427: </a>  }

<a name="line429">429: </a>  <a href="../../../docs/manualpages/Sys/PetscFree.html#PetscFree">PetscFree</a>(cpu_ulongs);
<a name="line430">430: </a>  *pctrl = ctrl;
<a name="line431">431: </a>  <font color="#4169E1">return</font>(0);
<a name="line432">432: </a>}

<a name="line434">434: </a><font color="#B22222">/*@C</font>
<a name="line435">435: </a><font color="#B22222">    <a href="../../../docs/manualpages/Sys/PetscOmpCtrlDestroy.html#PetscOmpCtrlDestroy">PetscOmpCtrlDestroy</a> - destroy the PETSc OpenMP controller</font>

<a name="line437">437: </a><font color="#B22222">    Input Parameter:</font>
<a name="line438">438: </a><font color="#B22222">.   pctrl  - a PETSc OpenMP controller</font>

<a name="line440">440: </a><font color="#B22222">    Level: developer</font>

<a name="line442">442: </a><font color="#B22222">.seealso <a href="../../../docs/manualpages/Sys/PetscOmpCtrlCreate.html#PetscOmpCtrlCreate">PetscOmpCtrlCreate</a>()</font>
<a name="line443">443: </a><font color="#B22222">@*/</font>
<a name="line444">444: </a><strong><font color="#4169E1"><a name="PetscOmpCtrlDestroy"></a><a href="../../../docs/manualpages/Sys/PetscErrorCode.html#PetscErrorCode">PetscErrorCode</a> <a href="../../../docs/manualpages/Sys/PetscOmpCtrlDestroy.html#PetscOmpCtrlDestroy">PetscOmpCtrlDestroy</a>(PetscOmpCtrl *pctrl)</font></strong>
<a name="line445">445: </a>{
<a name="line446">446: </a>  <a href="../../../docs/manualpages/Sys/PetscErrorCode.html#PetscErrorCode">PetscErrorCode</a>  ierr;
<a name="line447">447: </a>  PetscOmpCtrl    ctrl = *pctrl;

<a name="line450">450: </a>  hwloc_bitmap_free(ctrl-&gt;cpuset);
<a name="line451">451: </a>  hwloc_topology_destroy(ctrl-&gt;topology);
<a name="line452">452: </a>  PetscOmpCtrlDestroyBarrier(ctrl);
<a name="line453">453: </a>  <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Comm_free.html#MPI_Comm_free">MPI_Comm_free</a>(&amp;ctrl-&gt;omp_comm);
<a name="line454">454: </a>  <font color="#4169E1">if</font> (ctrl-&gt;is_omp_master) {
<a name="line455">455: </a>    hwloc_bitmap_free(ctrl-&gt;omp_cpuset);
<a name="line456">456: </a>    <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Comm_free.html#MPI_Comm_free">MPI_Comm_free</a>(&amp;ctrl-&gt;omp_master_comm);
<a name="line457">457: </a>  }
<a name="line458">458: </a>  <a href="../../../docs/manualpages/Sys/PetscFree.html#PetscFree">PetscFree</a>(ctrl);
<a name="line459">459: </a>  <font color="#4169E1">return</font>(0);
<a name="line460">460: </a>}

<a name="line462">462: </a><font color="#B22222">/*@C</font>
<a name="line463">463: </a><font color="#B22222">    <a href="../../../docs/manualpages/Sys/PetscOmpCtrlGetOmpComms.html#PetscOmpCtrlGetOmpComms">PetscOmpCtrlGetOmpComms</a> - Get MPI communicators from a PETSc OMP controller</font>

<a name="line465">465: </a><font color="#B22222">    Input Parameter:</font>
<a name="line466">466: </a><font color="#B22222">.   ctrl - a PETSc OMP controller</font>

<a name="line468">468: </a><font color="#B22222">    Output Parameter:</font>
<a name="line469">469: </a><font color="#B22222">+   omp_comm         - a communicator that includes a master rank and slave ranks where master spawns threads</font>
<a name="line470">470: </a><font color="#B22222">.   omp_master_comm  - on master ranks, return a communicator that include master ranks of each omp_comm;</font>
<a name="line471">471: </a><font color="#B22222">                       on slave ranks, MPI_COMM_NULL will be return in reality.</font>
<a name="line472">472: </a><font color="#B22222">-   is_omp_master    - true if the calling process is an OMP master rank.</font>

<a name="line474">474: </a><font color="#B22222">    Notes: any output parameter can be NULL. The parameter is just ignored.</font>

<a name="line476">476: </a><font color="#B22222">    Level: developer</font>
<a name="line477">477: </a><font color="#B22222">@*/</font>
<a name="line478">478: </a><strong><font color="#4169E1"><a name="PetscOmpCtrlGetOmpComms"></a><a href="../../../docs/manualpages/Sys/PetscErrorCode.html#PetscErrorCode">PetscErrorCode</a> <a href="../../../docs/manualpages/Sys/PetscOmpCtrlGetOmpComms.html#PetscOmpCtrlGetOmpComms">PetscOmpCtrlGetOmpComms</a>(PetscOmpCtrl ctrl,<a href="../../../docs/manualpages/Sys/MPI_Comm.html#MPI_Comm">MPI_Comm</a> *omp_comm,<a href="../../../docs/manualpages/Sys/MPI_Comm.html#MPI_Comm">MPI_Comm</a> *omp_master_comm,<a href="../../../docs/manualpages/Sys/PetscBool.html#PetscBool">PetscBool</a> *is_omp_master)</font></strong>
<a name="line479">479: </a>{
<a name="line481">481: </a>  <font color="#4169E1">if</font> (omp_comm)        *omp_comm        = ctrl-&gt;omp_comm;
<a name="line482">482: </a>  <font color="#4169E1">if</font> (omp_master_comm) *omp_master_comm = ctrl-&gt;omp_master_comm;
<a name="line483">483: </a>  <font color="#4169E1">if</font> (is_omp_master)   *is_omp_master   = ctrl-&gt;is_omp_master;
<a name="line484">484: </a>  <font color="#4169E1">return</font>(0);
<a name="line485">485: </a>}

<a name="line487">487: </a><font color="#B22222">/*@C</font>
<a name="line488">488: </a><font color="#B22222">    <a href="../../../docs/manualpages/Sys/PetscOmpCtrlBarrier.html#PetscOmpCtrlBarrier">PetscOmpCtrlBarrier</a> - Do barrier on MPI ranks in omp_comm contained by the PETSc OMP controller (to let slave ranks free their CPU)</font>

<a name="line490">490: </a><font color="#B22222">    Input Parameter:</font>
<a name="line491">491: </a><font color="#B22222">.   ctrl - a PETSc OMP controller</font>

<a name="line493">493: </a><font color="#B22222">    Notes:</font>
<a name="line494">494: </a><font color="#B22222">    this is a pthread barrier on MPI processes. Using <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Barrier.html#MPI_Barrier">MPI_Barrier</a> instead is conceptually correct. But MPI standard does not</font>
<a name="line495">495: </a><font color="#B22222">    require processes blocked by <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Barrier.html#MPI_Barrier">MPI_Barrier</a> free their CPUs to let other processes progress. In practice, to minilize latency,</font>
<a name="line496">496: </a><font color="#B22222">    MPI processes stuck in <a href="http://www.mpich.org/static/docs/latest/www3/MPI_Barrier.html#MPI_Barrier">MPI_Barrier</a> keep polling and do not free CPUs. In contrast, pthread_barrier has this requirement.</font>

<a name="line498">498: </a><font color="#B22222">    A code using <a href="../../../docs/manualpages/Sys/PetscOmpCtrlBarrier.html#PetscOmpCtrlBarrier">PetscOmpCtrlBarrier</a>() would be like this,</font>

<a name="line500">500: </a><font color="#B22222">    if (is_omp_master) {</font>
<a name="line501">501: </a><font color="#B22222">      <a href="../../../docs/manualpages/Sys/PetscOmpCtrlOmpRegionOnMasterBegin.html#PetscOmpCtrlOmpRegionOnMasterBegin">PetscOmpCtrlOmpRegionOnMasterBegin</a>(ctrl);</font>
<a name="line502">502: </a><font color="#B22222">      Call the library using OpenMP</font>
<a name="line503">503: </a><font color="#B22222">      <a href="../../../docs/manualpages/Sys/PetscOmpCtrlOmpRegionOnMasterEnd.html#PetscOmpCtrlOmpRegionOnMasterEnd">PetscOmpCtrlOmpRegionOnMasterEnd</a>(ctrl);</font>
<a name="line504">504: </a><font color="#B22222">    }</font>
<a name="line505">505: </a><font color="#B22222">    <a href="../../../docs/manualpages/Sys/PetscOmpCtrlBarrier.html#PetscOmpCtrlBarrier">PetscOmpCtrlBarrier</a>(ctrl);</font>

<a name="line507">507: </a><font color="#B22222">    Level: developer</font>

<a name="line509">509: </a><font color="#B22222">.seealso <a href="../../../docs/manualpages/Sys/PetscOmpCtrlOmpRegionOnMasterBegin.html#PetscOmpCtrlOmpRegionOnMasterBegin">PetscOmpCtrlOmpRegionOnMasterBegin</a>(), <a href="../../../docs/manualpages/Sys/PetscOmpCtrlOmpRegionOnMasterEnd.html#PetscOmpCtrlOmpRegionOnMasterEnd">PetscOmpCtrlOmpRegionOnMasterEnd</a>()</font>
<a name="line510">510: </a><font color="#B22222">@*/</font>
<a name="line511">511: </a><strong><font color="#4169E1"><a name="PetscOmpCtrlBarrier"></a><a href="../../../docs/manualpages/Sys/PetscErrorCode.html#PetscErrorCode">PetscErrorCode</a> <a href="../../../docs/manualpages/Sys/PetscOmpCtrlBarrier.html#PetscOmpCtrlBarrier">PetscOmpCtrlBarrier</a>(PetscOmpCtrl ctrl)</font></strong>
<a name="line512">512: </a>{

<a name="line516">516: </a>  pthread_barrier_wait(ctrl-&gt;barrier);
<a name="line517">517: </a>  <font color="#4169E1">if</font> (ierr &amp;&amp; ierr != PTHREAD_BARRIER_SERIAL_THREAD) <a href="../../../docs/manualpages/Sys/SETERRQ1.html#SETERRQ1">SETERRQ1</a>(<a href="../../../docs/manualpages/Sys/PETSC_COMM_SELF.html#PETSC_COMM_SELF">PETSC_COMM_SELF</a>,PETSC_ERR_LIB,<font color="#666666">"pthread_barrier_wait failed within <a href="../../../docs/manualpages/Sys/PetscOmpCtrlBarrier.html#PetscOmpCtrlBarrier">PetscOmpCtrlBarrier</a> with return code %D\n"</font>, ierr);
<a name="line518">518: </a>  <font color="#4169E1">return</font>(0);
<a name="line519">519: </a>}

<a name="line521">521: </a><font color="#B22222">/*@C</font>
<a name="line522">522: </a><font color="#B22222">    <a href="../../../docs/manualpages/Sys/PetscOmpCtrlOmpRegionOnMasterBegin.html#PetscOmpCtrlOmpRegionOnMasterBegin">PetscOmpCtrlOmpRegionOnMasterBegin</a> - Mark the beginning of an OpenMP library call on master ranks</font>

<a name="line524">524: </a><font color="#B22222">    Input Parameter:</font>
<a name="line525">525: </a><font color="#B22222">.   ctrl - a PETSc OMP controller</font>

<a name="line527">527: </a><font color="#B22222">    Notes:</font>
<a name="line528">528: </a><font color="#B22222">    Only master ranks can call this function. Call <a href="../../../docs/manualpages/Sys/PetscOmpCtrlGetOmpComms.html#PetscOmpCtrlGetOmpComms">PetscOmpCtrlGetOmpComms</a>() to know if this is a master rank.</font>
<a name="line529">529: </a><font color="#B22222">    This function changes CPU binding of master ranks and nthreads-var of OpenMP runtime</font>

<a name="line531">531: </a><font color="#B22222">    Level: developer</font>

<a name="line533">533: </a><font color="#B22222">.seealso: <a href="../../../docs/manualpages/Sys/PetscOmpCtrlOmpRegionOnMasterEnd.html#PetscOmpCtrlOmpRegionOnMasterEnd">PetscOmpCtrlOmpRegionOnMasterEnd</a>()</font>
<a name="line534">534: </a><font color="#B22222">@*/</font>
<a name="line535">535: </a><strong><font color="#4169E1"><a name="PetscOmpCtrlOmpRegionOnMasterBegin"></a><a href="../../../docs/manualpages/Sys/PetscErrorCode.html#PetscErrorCode">PetscErrorCode</a> <a href="../../../docs/manualpages/Sys/PetscOmpCtrlOmpRegionOnMasterBegin.html#PetscOmpCtrlOmpRegionOnMasterBegin">PetscOmpCtrlOmpRegionOnMasterBegin</a>(PetscOmpCtrl ctrl)</font></strong>
<a name="line536">536: </a>{

<a name="line540">540: </a>  hwloc_set_cpubind(ctrl-&gt;topology,ctrl-&gt;omp_cpuset,HWLOC_CPUBIND_PROCESS);
<a name="line541">541: </a>  omp_set_num_threads(ctrl-&gt;omp_comm_size); <font color="#B22222">/* may override the OMP_NUM_THREAD env var */</font>
<a name="line542">542: </a>  <font color="#4169E1">return</font>(0);
<a name="line543">543: </a>}

<a name="line545">545: </a><font color="#B22222">/*@C</font>
<a name="line546">546: </a><font color="#B22222">   <a href="../../../docs/manualpages/Sys/PetscOmpCtrlOmpRegionOnMasterEnd.html#PetscOmpCtrlOmpRegionOnMasterEnd">PetscOmpCtrlOmpRegionOnMasterEnd</a> - Mark the end of an OpenMP library call on master ranks</font>

<a name="line548">548: </a><font color="#B22222">   Input Parameter:</font>
<a name="line549">549: </a><font color="#B22222">.  ctrl - a PETSc OMP controller</font>

<a name="line551">551: </a><font color="#B22222">   Notes:</font>
<a name="line552">552: </a><font color="#B22222">   Only master ranks can call this function. Call <a href="../../../docs/manualpages/Sys/PetscOmpCtrlGetOmpComms.html#PetscOmpCtrlGetOmpComms">PetscOmpCtrlGetOmpComms</a>() to know if this is a master rank.</font>
<a name="line553">553: </a><font color="#B22222">   This function restores the CPU binding of master ranks and set and nthreads-var of OpenMP runtime to 1.</font>

<a name="line555">555: </a><font color="#B22222">   Level: developer</font>

<a name="line557">557: </a><font color="#B22222">.seealso: <a href="../../../docs/manualpages/Sys/PetscOmpCtrlOmpRegionOnMasterBegin.html#PetscOmpCtrlOmpRegionOnMasterBegin">PetscOmpCtrlOmpRegionOnMasterBegin</a>()</font>
<a name="line558">558: </a><font color="#B22222">@*/</font>
<a name="line559">559: </a><strong><font color="#4169E1"><a name="PetscOmpCtrlOmpRegionOnMasterEnd"></a><a href="../../../docs/manualpages/Sys/PetscErrorCode.html#PetscErrorCode">PetscErrorCode</a> <a href="../../../docs/manualpages/Sys/PetscOmpCtrlOmpRegionOnMasterEnd.html#PetscOmpCtrlOmpRegionOnMasterEnd">PetscOmpCtrlOmpRegionOnMasterEnd</a>(PetscOmpCtrl ctrl)</font></strong>
<a name="line560">560: </a>{

<a name="line564">564: </a>  hwloc_set_cpubind(ctrl-&gt;topology,ctrl-&gt;cpuset,HWLOC_CPUBIND_PROCESS);
<a name="line565">565: </a>  omp_set_num_threads(1);
<a name="line566">566: </a>  <font color="#4169E1">return</font>(0);
<a name="line567">567: </a>}

<a name="line569">569: </a><strong><font color="#228B22">#undef USE_MMAP_ALLOCATE_SHARED_MEMORY</font></strong>
<a name="line570">570: </a><font color="#A020F0">#endif </font><font color="#B22222">/* defined(PETSC_HAVE_OPENMP_SUPPORT) */</font><font color="#A020F0"></font>
</pre>
</body>

</html>
